{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook walks through creating, setting up and training a Deep Markov Model on a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiyunqiu/miniconda3/envs/myenv/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#General Purpose Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, os, sys, time\n",
    "sys.path.append('/Users/shiyunqiu/dmm/')\n",
    "from utils.misc import getConfigFile, readPickle, displayTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A] Data\n",
    "* The polyphonic music code (which can be run from the `expt` folder) represents an example of running on binary data\n",
    "* There is code to create a synthetic dataset in `dmm_data/load.py`, we will load that dataset here (it will be created the first time this function is called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating linear matrices\n",
      "Creating linear matrices\n",
      "Saved  1  objects\n",
      "Saving...\n",
      "<type 'dict'> ['test', 'dim_observations', 'train', 'valid', 'data_type']\n"
     ]
    }
   ],
   "source": [
    "#Import load function to load synthetic data\n",
    "# from dmm_data.load import load\n",
    "# dataset = load('synthetic')\n",
    "# print type(dataset), dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Dataset format\n",
    "The dataset's are expected to be in a specific format:\n",
    "* The type of the variable `dataset` is `dict`\n",
    "* The keys of this dataset correspond to parameters used by the model as well as raw data\n",
    "* `dim_observations` is of type `int` and it corresponds to the dimensionality of the raw data at each point in time\n",
    "* `data_type` can be `binary` or `real`\n",
    "* `train`,`valid` and `test` are dictionaries that house the pre-split data. `dataset['train']` contains three items: \n",
    "    * `dataset['train']['tensor']` is a 3D tensor where the dimensions correspond to `Nsamples x T x dim_observations`\n",
    "    * `dataset['train']['mask']` is a 2D matrix where the dimensions correspond to `Nsamples x T`. Each entry in this matrix corresponds to whether a sample was observed at that point in time. This is used internally by the model to model variable length sequences. \n",
    "    * `dataset['train']['tensor_Z']` is not used by the model. It corresponds to the states of the **true** underlying latent variables that created this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the observations:  3\n",
      "Data type of features: real\n",
      "dtype:  train  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (10000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (10000, 10)), ('tensor', <type 'numpy.ndarray'>, (10000, 10, 3))]\n",
      "--------\n",
      "\n",
      "dtype:  valid  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (1000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (1000, 10)), ('tensor', <type 'numpy.ndarray'>, (1000, 10, 3))]\n",
      "--------\n",
      "\n",
      "dtype:  test  type(dataset[dtype]):  <type 'dict'>\n",
      "[('tensor_Z', <type 'numpy.ndarray'>, (1000, 10, 3)), ('mask', <type 'numpy.ndarray'>, (1000, 10)), ('tensor', <type 'numpy.ndarray'>, (1000, 10, 3))]\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print 'Dimensionality of the observations: ', dataset['dim_observations']\n",
    "# print 'Data type of features:', dataset['data_type']\n",
    "# for dtype in ['train','valid','test']:\n",
    "#     print 'dtype: ',dtype, ' type(dataset[dtype]): ',type(dataset[dtype])\n",
    "#     print [(k,type(dataset[dtype][k]), dataset[dtype][k].shape) for k in dataset[dtype]]\n",
    "#     print '--------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Creating your own dataset\n",
    "* When creating your own data, it should have the following structure: \n",
    "    * type: dict\n",
    "        * `dim_observations`: int\n",
    "        * `data_type`: `binary` or `real`\n",
    "        * `train`, `valid`, `test` must be dictionaries with the following keys: \n",
    "            * `tensor` : Raw data as a 3D tensor with dimensions `Nsamples x T x dim_observations`\n",
    "            * `mask`   : Mask for the raw data as a 2D matrix with dimensions `Nsamples x T`\n",
    "* Now that we have the dataset in the desired format, lets look at setting up the model, we'll first load the necessary files to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/shiyunqiu/AM231_Project/data/bitcoin_price.csv')\n",
    "df = df[::-1]\n",
    "df = df.Close.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df) * 0.8)\n",
    "valid_size = int(len(df) * 0.1)\n",
    "test_size = len(df) - train_size - valid_size\n",
    "# train = df[0:train_size, :]\n",
    "# valid = df[train_size:train_size+valid_size, :]\n",
    "# test = df[train_size+valid_size:len(dataset), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(T, nsample, start_indx, end_indx):\n",
    "    sample = []\n",
    "    idx = np.random.randint(start_indx, end_indx, nsample, 'int')\n",
    "    for i, val in enumerate(idx):\n",
    "        piece = df[val:val+10, :]\n",
    "        sample.append(piece)\n",
    "    return np.array(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: T=10, 1000 samples\n",
    "train = generate_sample(10, 1000, 0, train_size-10)\n",
    "\n",
    "# Valid: T=10, 100 samples\n",
    "valid = generate_sample(10, 100, train_size, train_size+valid_size-10)\n",
    "\n",
    "# Test: T=10, 100 samples\n",
    "test = generate_sample(10, 100, train_size+valid_size, len(df)-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "train_dict['tensor'] = train\n",
    "train_dict['mask'] = np.ones((1000, 10))\n",
    "\n",
    "valid_dict = {}\n",
    "valid_dict['tensor'] = valid\n",
    "valid_dict['mask'] = np.ones((100, 10))\n",
    "\n",
    "test_dict = {}\n",
    "test_dict['tensor'] = test\n",
    "test_dict['mask'] = np.ones((100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['dim_observations'] = 1\n",
    "dataset['data_type'] = 'real'\n",
    "dataset['train'] = train_dict\n",
    "dataset['valid'] = valid_dict\n",
    "dataset['test'] = test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the observations:  1\n",
      "Data type of features: real\n",
      "dtype:  train  type(dataset[dtype]):  <type 'dict'>\n",
      "[('mask', <type 'numpy.ndarray'>, (1000, 10)), ('tensor', <type 'numpy.ndarray'>, (1000, 10, 1))]\n",
      "--------\n",
      "\n",
      "dtype:  valid  type(dataset[dtype]):  <type 'dict'>\n",
      "[('mask', <type 'numpy.ndarray'>, (100, 10)), ('tensor', <type 'numpy.ndarray'>, (100, 10, 1))]\n",
      "--------\n",
      "\n",
      "dtype:  test  type(dataset[dtype]):  <type 'dict'>\n",
      "[('mask', <type 'numpy.ndarray'>, (100, 10)), ('tensor', <type 'numpy.ndarray'>, (100, 10, 1))]\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Dimensionality of the observations: ', dataset['dim_observations']\n",
    "print 'Data type of features:', dataset['data_type']\n",
    "for dtype in ['train','valid','test']:\n",
    "    print 'dtype: ',dtype, ' type(dataset[dtype]): ',type(dataset[dtype])\n",
    "    print [(k,type(dataset[dtype][k]), dataset[dtype][k].shape) for k in dataset[dtype]]\n",
    "    print '--------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t< importing DMM > took  2.15572285652   seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from   model_th.dmm import DMM\n",
    "import model_th.learning as DMM_learn\n",
    "import model_th.evaluate as DMM_evaluate\n",
    "displayTime('importing DMM',start_time, time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## B] Model Hyperparameters\n",
    "* To setup the model, we need to specify the hyperparameters\n",
    "* Normally, if you were running from a script, you would run the following: `from parse_args import params` (e.g `expt/train.py` within the script.\n",
    "* This lets you specify hyperparameters for the model via the command line. See the shell scripts in the folder `expt/` for an example of this. \n",
    "* Since we're in Ipython, we'll reload a saved version of `params` and see what the default values currently are. To know more about how the choices of hyperparameters affect the model, you can run `python parse_args.py -h` in the main directory.\n",
    "* The `unique_id` is created based on the default parameters or those specified via the commend line.\n",
    "* The parameters (`data_type` and `dim_observations`) of the dataset need to be incorporated into `params` for the model to be able to setup the weights appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read  1  objects\n",
      "dataset \tmm\n",
      "epochs \t2000\n",
      "seed \t1\n",
      "init_weight \t0.1\n",
      "dim_stochastic \t100\n",
      "expt_name \tuid\n",
      "reg_value \t0.05\n",
      "reloadFile \t./NOSUCHFILE\n",
      "reg_spec \t_\n",
      "dim_hidden \t200\n",
      "lr \t0.0008\n",
      "reg_type \tl2\n",
      "init_scheme \tuniform\n",
      "optimizer \tadam\n",
      "use_generative_prior \tapprox\n",
      "maxout_stride \t4\n",
      "batch_size \t20\n",
      "savedir \t./chkpt\n",
      "forget_bias \t-5.0\n",
      "inference_model \tR\n",
      "emission_layers \t2\n",
      "savefreq \t10\n",
      "rnn_cell \tlstm\n",
      "rnn_size \t600\n",
      "paramFile \t./NOSUCHFILE\n",
      "nonlinearity \trelu\n",
      "rnn_dropout \t0.1\n",
      "transition_layers \t2\n",
      "anneal_rate \t2.0\n",
      "debug \tFalse\n",
      "validate_only \tFalse\n",
      "transition_type \tmlp\n",
      "unique_id \tDMM_lr-0_0008-dh-200-ds-100-nl-relu-bs-20-ep-2000-rs-600-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid\n",
      "leaky_param \t0.0\n"
     ]
    }
   ],
   "source": [
    "params = readPickle('../default.pkl')[0]\n",
    "for k in params:\n",
    "    print k, '\\t',params[k]\n",
    "params['data_type'] = dataset['data_type']\n",
    "params['dim_observations'] = dataset['dim_observations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C] Building the DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint prefix:  ./chkpt-ipython//DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-config.pkl\n",
      "\t<<Sampling biases for LSTM from exponential distribution>>\n",
      "\t<<Nparameters: 14410>>\n",
      "\t<<WARNING: lr will not differentiated with respect to>>\n",
      "\t<<WARNING: anneal will not differentiated with respect to>>\n",
      "\t<<WARNING: update_ctr will not differentiated with respect to>>\n",
      "\t<<Anneal = 1 in 2.0 param. updates>>\n",
      "\t<<Building with RNN dropout:0.1>>\n",
      "\t<<In _LSTM_RNN_layer with dropout 0.1000>>\n",
      "\t<<Modifying : [q_W_input_0,q_b_input_0,W_lstm_r,b_lstm_r,U_lstm_r,q_W_st,q_b_st,q_W_mu,q_b_mu,q_W_cov,q_b_cov,p_trans_W_0,p_trans_b_0,p_trans_W_1,p_trans_b_1,p_trans_W_mu,p_trans_b_mu,p_trans_W_cov,p_trans_b_cov,p_emis_W_0,p_emis_b_0,p_emis_W_1,p_emis_b_1,p_emis_W_out,p_emis_b_out]>>\n",
      "<< Reg:(l2) Reg. Val:(0.05) Reg. Spec.:(_)>>\n",
      "<<<<<< Adding l2 regularization for q_W_input_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_input_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for W_lstm_r >>>>>>\n",
      "<<<<<< Adding l2 regularization for b_lstm_r >>>>>>\n",
      "<<<<<< Adding l2 regularization for U_lstm_r >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_W_st >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_st >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_W_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_W_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for q_b_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_mu >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_W_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_trans_b_cov >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_W_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_b_0 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_W_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_b_1 >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_W_out >>>>>>\n",
      "<<<<<< Adding l2 regularization for p_emis_b_out >>>>>>\n",
      "<<<<<< Normalizing Gradients to have norm ( 1.0 ) >>>>>>\n",
      "\t<<0 other updates>>\n",
      "\t<<Building with RNN dropout:0.0>>\n",
      "\t<<In _LSTM_RNN_layer with dropout 0.0000>>\n",
      "\t<<Completed DMM setup>>\n",
      "\t<<_buildModel took : 147.2028 seconds>>\n"
     ]
    }
   ],
   "source": [
    "#The dataset is small, lets change some of the default parameters and the unique ID\n",
    "params['dim_stochastic'] = 2\n",
    "params['dim_hidden']     = 20\n",
    "params['rnn_size']       = 40\n",
    "params['epochs']         = 40\n",
    "params['batch_size']     = 200\n",
    "params['unique_id'] = params['unique_id'].replace('ds-100','ds-2').replace('dh-200','dh-20').replace('rs-600','rs-40')\n",
    "params['unique_id'] = params['unique_id'].replace('ep-2000','ep-40').replace('bs-20','bs-200')\n",
    "\n",
    "#Create a temporary directory to save checkpoints\n",
    "params['savedir']   = params['savedir']+'-ipython/'\n",
    "os.system('mkdir -p '+params['savedir'])\n",
    "\n",
    "#Specify the file where `params` corresponding for this choice of model and data will be saved\n",
    "pfile= params['savedir']+'/'+params['unique_id']+'-config.pkl'\n",
    "\n",
    "print 'Checkpoint prefix: ', pfile\n",
    "dmm  = DMM(params, paramFile = pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D] Parameter Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<<Original dim: [3 5 1],[3 5]>>\n",
      "\t<<New dim: [1000   10    1],[1000   10]>>\n",
      "\t<<Bnum: 0, Batch Bound: 121066.0698, |w|: 22.2577, |dw|: 1.0000, |w_opt|: 0.0000>>\n",
      "\t<<-veCLL:242132138.8413, KL:82.8648, anneal:0.0100>>\n",
      "\t<<(Ep 0) Bound: 121748.4526 [Took 0.3633 seconds] >>\n",
      "\t<<Saving at epoch 0>>\n",
      "\t<<Saved model (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP0-params) \n",
      "\t\t opt (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP0-optParams) weights>>\n",
      "\t<<Original dim: [1000   10    1],[1000   10]>>\n",
      "\t<<New dim: [100  10   1],[100  10]>>\n",
      "\t<<(Evaluate) Validation Bound: 2477875.8306 [Took 0.0122 seconds]>>\n",
      "\t<<Original dim: [100  10   1],[100  10]>>\n",
      "\t<<New dim: [1000   10    1],[1000   10]>>\n",
      "\t<<Bnum: 0, Batch Bound: 123485.7700, |w|: 22.2575, |dw|: 1.0000, |w_opt|: 0.4095>>\n",
      "\t<<-veCLL:246971439.8253, KL:100.0808, anneal:1.0000>>\n",
      "\t<<(Ep 1) Bound: 120229.1339 [Took 0.4657 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 119762.4573, |w|: 22.2575, |dw|: 1.0000, |w_opt|: 0.6512>>\n",
      "\t<<-veCLL:239524776.4104, KL:138.2605, anneal:1.0000>>\n",
      "\t<<(Ep 2) Bound: 118618.0500 [Took 0.4915 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 111038.6617, |w|: 22.2574, |dw|: 1.0000, |w_opt|: 0.7937>>\n",
      "\t<<-veCLL:222077136.7003, KL:186.7002, anneal:1.0000>>\n",
      "\t<<(Ep 3) Bound: 116948.9790 [Took 0.5541 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 109669.5155, |w|: 22.2576, |dw|: 1.0000, |w_opt|: 0.8778>>\n",
      "\t<<-veCLL:219338783.8051, KL:247.1382, anneal:1.0000>>\n",
      "\t<<(Ep 4) Bound: 115102.3994 [Took 0.3889 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 104059.4964, |w|: 22.2585, |dw|: 1.0000, |w_opt|: 0.9274>>\n",
      "\t<<-veCLL:208118666.1562, KL:326.6645, anneal:1.0000>>\n",
      "\t<<(Ep 5) Bound: 113096.3388 [Took 0.4355 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 126611.7988, |w|: 22.2601, |dw|: 1.0000, |w_opt|: 0.9564>>\n",
      "\t<<-veCLL:253223122.5924, KL:475.0230, anneal:1.0000>>\n",
      "\t<<(Ep 6) Bound: 110934.3507 [Took 0.3796 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 115846.1563, |w|: 22.2629, |dw|: 1.0000, |w_opt|: 0.9732>>\n",
      "\t<<-veCLL:231691636.4981, KL:676.0330, anneal:1.0000>>\n",
      "\t<<(Ep 7) Bound: 108182.6844 [Took 0.4300 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 98184.1778, |w|: 22.2670, |dw|: 1.0000, |w_opt|: 0.9821>>\n",
      "\t<<-veCLL:196367373.6981, KL:981.8318, anneal:1.0000>>\n",
      "\t<<(Ep 8) Bound: 104745.4769 [Took 0.4898 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 111136.6845, |w|: 22.2731, |dw|: 1.0000, |w_opt|: 0.9862>>\n",
      "\t<<-veCLL:222271988.1590, KL:1380.8356, anneal:1.0000>>\n",
      "\t<<(Ep 9) Bound: 100813.5804 [Took 0.4885 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 97844.2791, |w|: 22.2820, |dw|: 1.0000, |w_opt|: 0.9879>>\n",
      "\t<<-veCLL:195686590.9941, KL:1967.1415, anneal:1.0000>>\n",
      "\t<<(Ep 10) Bound: 95749.3846 [Took 0.4424 seconds] >>\n",
      "\t<<Saving at epoch 10>>\n",
      "\t<<Saved model (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP10-params) \n",
      "\t\t opt (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP10-optParams) weights>>\n",
      "\t<<Original dim: [1000   10    1],[1000   10]>>\n",
      "\t<<New dim: [100  10   1],[100  10]>>\n",
      "\t<<(Evaluate) Validation Bound: 1901893.4145 [Took 0.0099 seconds]>>\n",
      "\t<<Original dim: [100  10   1],[100  10]>>\n",
      "\t<<New dim: [1000   10    1],[1000   10]>>\n",
      "\t<<Bnum: 0, Batch Bound: 90992.8836, |w|: 22.2937, |dw|: 1.0000, |w_opt|: 0.9879>>\n",
      "\t<<-veCLL:181983174.0016, KL:2593.1739, anneal:1.0000>>\n",
      "\t<<(Ep 11) Bound: 89796.0641 [Took 0.4649 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 86165.7594, |w|: 22.3079, |dw|: 1.0000, |w_opt|: 0.9854>>\n",
      "\t<<-veCLL:172328064.9149, KL:3453.8304, anneal:1.0000>>\n",
      "\t<<(Ep 12) Bound: 82803.0065 [Took 0.3631 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 76678.6360, |w|: 22.3245, |dw|: 1.0000, |w_opt|: 0.9833>>\n",
      "\t<<-veCLL:153352881.2809, KL:4390.8150, anneal:1.0000>>\n",
      "\t<<(Ep 13) Bound: 73929.0423 [Took 0.5813 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 64765.6603, |w|: 22.3430, |dw|: 1.0000, |w_opt|: 0.9777>>\n",
      "\t<<-veCLL:129525550.4083, KL:5770.1818, anneal:1.0000>>\n",
      "\t<<(Ep 14) Bound: 63729.9681 [Took 0.5211 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 54933.3493, |w|: 22.3637, |dw|: 1.0000, |w_opt|: 0.9739>>\n",
      "\t<<-veCLL:109859486.6604, KL:7211.9968, anneal:1.0000>>\n",
      "\t<<(Ep 15) Bound: 53349.5578 [Took 0.4083 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 47825.7224, |w|: 22.3860, |dw|: 1.0000, |w_opt|: 0.9747>>\n",
      "\t<<-veCLL:95642812.8385, KL:8631.9308, anneal:1.0000>>\n",
      "\t<<(Ep 16) Bound: 42148.0946 [Took 0.4250 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 33077.1565, |w|: 22.4099, |dw|: 1.0000, |w_opt|: 0.9776>>\n",
      "\t<<-veCLL:66144715.3125, KL:9597.7008, anneal:1.0000>>\n",
      "\t<<(Ep 17) Bound: 31282.0382 [Took 0.4354 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 22548.8512, |w|: 22.4349, |dw|: 1.0000, |w_opt|: 0.9745>>\n",
      "\t<<-veCLL:45087161.5339, KL:10540.8341, anneal:1.0000>>\n",
      "\t<<(Ep 18) Bound: 21842.9366 [Took 0.4953 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 17962.9688, |w|: 22.4615, |dw|: 1.0000, |w_opt|: 0.9768>>\n",
      "\t<<-veCLL:35915218.7227, KL:10718.8279, anneal:1.0000>>\n",
      "\t<<(Ep 19) Bound: 13629.1814 [Took 0.4079 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 9427.4969, |w|: 22.4896, |dw|: 1.0000, |w_opt|: 0.9713>>\n",
      "\t<<-veCLL:18844690.5814, KL:10303.1680, anneal:1.0000>>\n",
      "\t<<(Ep 20) Bound: 7813.3024 [Took 0.4085 seconds] >>\n",
      "\t<<Saving at epoch 20>>\n",
      "\t<<Saved model (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP20-params) \n",
      "\t\t opt (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP20-optParams) weights>>\n",
      "\t<<Original dim: [1000   10    1],[1000   10]>>\n",
      "\t<<New dim: [100  10   1],[100  10]>>\n",
      "\t<<(Evaluate) Validation Bound: 119559.4677 [Took 0.0122 seconds]>>\n",
      "\t<<Original dim: [100  10   1],[100  10]>>\n",
      "\t<<New dim: [1000   10    1],[1000   10]>>\n",
      "\t<<Bnum: 0, Batch Bound: 5871.4122, |w|: 22.5200, |dw|: 1.0000, |w_opt|: 0.9782>>\n",
      "\t<<-veCLL:11733864.5610, KL:8959.8758, anneal:1.0000>>\n",
      "\t<<(Ep 21) Bound: 4318.5577 [Took 0.4317 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 2592.7220, |w|: 22.5531, |dw|: 1.0000, |w_opt|: 0.9782>>\n",
      "\t<<-veCLL:5178144.4774, KL:7299.5630, anneal:1.0000>>\n",
      "\t<<(Ep 22) Bound: 2169.6771 [Took 0.4876 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 1368.2473, |w|: 22.5871, |dw|: 1.0000, |w_opt|: 0.9829>>\n",
      "\t<<-veCLL:2730873.4160, KL:5621.1563, anneal:1.0000>>\n",
      "\t<<(Ep 23) Bound: 1064.5407 [Took 0.5484 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 680.2889, |w|: 22.6209, |dw|: 1.0000, |w_opt|: 0.9877>>\n",
      "\t<<-veCLL:1355871.1236, KL:4706.6170, anneal:1.0000>>\n",
      "\t<<(Ep 24) Bound: 498.2999 [Took 0.4829 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 328.8661, |w|: 22.6522, |dw|: 1.0000, |w_opt|: 0.9887>>\n",
      "\t<<-veCLL:653194.8243, KL:4537.3128, anneal:1.0000>>\n",
      "\t<<(Ep 25) Bound: 237.2289 [Took 0.4805 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 145.1315, |w|: 22.6759, |dw|: 1.0000, |w_opt|: 0.9918>>\n",
      "\t<<-veCLL:285314.8826, KL:4948.1037, anneal:1.0000>>\n",
      "\t<<(Ep 26) Bound: 111.7774 [Took 0.4539 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 67.0708, |w|: 22.6800, |dw|: 1.0000, |w_opt|: 0.9859>>\n",
      "\t<<-veCLL:128527.4734, KL:5614.0510, anneal:1.0000>>\n",
      "\t<<(Ep 27) Bound: 51.1037 [Took 0.5561 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 38.7126, |w|: 22.6596, |dw|: 1.0000, |w_opt|: 0.9894>>\n",
      "\t<<-veCLL:71463.1485, KL:5962.1211, anneal:1.0000>>\n",
      "\t<<(Ep 28) Bound: 29.6909 [Took 0.4101 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 17.8549, |w|: 22.6120, |dw|: 1.0000, |w_opt|: 0.9920>>\n",
      "\t<<-veCLL:29399.3428, KL:6310.4034, anneal:1.0000>>\n",
      "\t<<(Ep 29) Bound: 18.7305 [Took 0.6064 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 16.3041, |w|: 22.5030, |dw|: 1.0000, |w_opt|: 0.9930>>\n",
      "\t<<-veCLL:25909.9176, KL:6698.2632, anneal:1.0000>>\n",
      "\t<<(Ep 30) Bound: 14.4713 [Took 0.6638 seconds] >>\n",
      "\t<<Saving at epoch 30>>\n",
      "\t<<Saved model (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP30-params) \n",
      "\t\t opt (./chkpt-ipython/DMM_lr-0_0008-dh-20-ds-2-nl-relu-bs-200-ep-40-rs-40-rd-0_1-infm-R-tl-2-el-2-ar-2_0-use_p-approx-rc-lstm-uid-EP30-optParams) weights>>\n",
      "\t<<Original dim: [1000   10    1],[1000   10]>>\n",
      "\t<<New dim: [100  10   1],[100  10]>>\n",
      "\t<<(Evaluate) Validation Bound: 35.6094 [Took 0.0095 seconds]>>\n",
      "\t<<Original dim: [100  10   1],[100  10]>>\n",
      "\t<<New dim: [1000   10    1],[1000   10]>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<<Bnum: 0, Batch Bound: 13.0916, |w|: 22.3120, |dw|: 1.0000, |w_opt|: 0.9776>>\n",
      "\t<<-veCLL:19480.5608, KL:6702.7217, anneal:1.0000>>\n",
      "\t<<(Ep 31) Bound: 13.2474 [Took 0.5553 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 13.2991, |w|: 22.0427, |dw|: 1.0000, |w_opt|: 0.7308>>\n",
      "\t<<-veCLL:20296.8739, KL:6301.4260, anneal:1.0000>>\n",
      "\t<<(Ep 32) Bound: 12.8589 [Took 0.5022 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 12.4563, |w|: 21.7881, |dw|: 1.0000, |w_opt|: 0.5485>>\n",
      "\t<<-veCLL:19326.7538, KL:5585.7974, anneal:1.0000>>\n",
      "\t<<(Ep 33) Bound: 12.9108 [Took 0.4604 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 11.2107, |w|: 21.6209, |dw|: 1.0000, |w_opt|: 0.5139>>\n",
      "\t<<-veCLL:18331.9170, KL:4089.5183, anneal:1.0000>>\n",
      "\t<<(Ep 34) Bound: 11.7725 [Took 0.5826 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 11.6001, |w|: 21.5315, |dw|: 1.0000, |w_opt|: 0.4322>>\n",
      "\t<<-veCLL:19555.7342, KL:3644.5512, anneal:1.0000>>\n",
      "\t<<(Ep 35) Bound: 10.9250 [Took 0.4263 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 10.5353, |w|: 21.4828, |dw|: 1.0000, |w_opt|: 0.3986>>\n",
      "\t<<-veCLL:17837.9307, KL:3232.7501, anneal:1.0000>>\n",
      "\t<<(Ep 36) Bound: 11.2919 [Took 0.4571 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 10.4306, |w|: 21.4439, |dw|: 1.0000, |w_opt|: 0.4625>>\n",
      "\t<<-veCLL:17419.5649, KL:3441.5808, anneal:1.0000>>\n",
      "\t<<(Ep 37) Bound: 10.3490 [Took 0.4625 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 10.5091, |w|: 21.4081, |dw|: 1.0000, |w_opt|: 0.3396>>\n",
      "\t<<-veCLL:17931.8769, KL:3086.2768, anneal:1.0000>>\n",
      "\t<<(Ep 38) Bound: 10.3479 [Took 0.4528 seconds] >>\n",
      "\t<<Bnum: 0, Batch Bound: 9.7559, |w|: 21.3793, |dw|: 1.0000, |w_opt|: 0.3282>>\n",
      "\t<<-veCLL:16389.7009, KL:3122.1758, anneal:1.0000>>\n",
      "\t<<(Ep 39) Bound: 9.9675 [Took 0.4435 seconds] >>\n"
     ]
    }
   ],
   "source": [
    "#savef specifies the prefix for the checkpoints - we'll use the same save directory as before \n",
    "savef    = os.path.join(params['savedir'],params['unique_id'])\n",
    "savedata = DMM_learn.learn(dmm, dataset['train'], epoch_start =0 ,\n",
    "                                epoch_end = params['epochs'],\n",
    "                                batch_size = 200,\n",
    "                                savefreq   = params['savefreq'],\n",
    "                                savefile   = savef,\n",
    "                                dataset_eval=dataset['valid'],\n",
    "                                shuffle    = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
