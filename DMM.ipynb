{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An implementation of a Deep Markov Model in Pyro based on reference [1].\n",
    "This is essentially the DKS variant outlined in the paper. The primary difference\n",
    "between this implementation and theirs is that in our version any KL divergence terms\n",
    "in the ELBO are estimated via sampling, while they make use of the analytic formulae.\n",
    "We also illustrate the use of normalizing flows in the variational distribution (in which\n",
    "case analytic formulae for the KL divergences are in any case unavailable).\n",
    "Reference:\n",
    "[1] Structured Inference Networks for Nonlinear State Space Models [arXiv:1609.09869]\n",
    "    Rahul G. Krishnan, Uri Shalit, David Sontag\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pyro\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import ClippedAdam\n",
    "import pyro.distributions as dist\n",
    "from pyro.util import ng_ones\n",
    "from pyro.distributions.transformed_distribution import InverseAutoregressiveFlow\n",
    "from pyro.distributions.transformed_distribution import TransformedDistribution\n",
    "import six.moves.cPickle as pickle\n",
    "from os.path import exists\n",
    "import argparse\n",
    "import time\n",
    "from util import get_logger\n",
    "\n",
    "\n",
    "class Emitter(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the bernoulli observation likelihood `p(x_t | z_t)`\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, z_dim, emission_dim):\n",
    "        super(Emitter, self).__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z_t):\n",
    "        \"\"\"\n",
    "        Given the latent z at a particular time step t we return the vector of\n",
    "        probabilities `ps` that parameterizes the bernoulli distribution `p(x_t|z_t)`\n",
    "        \"\"\"\n",
    "        h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
    "        ps = self.sigmoid(self.lin_hidden_to_input(h2))\n",
    "        return ps\n",
    "\n",
    "\n",
    "class GatedTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the gaussian latent transition probability `p(z_t | z_{t-1})`\n",
    "    See section 5 in the reference for comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super(GatedTransition, self).__init__()\n",
    "        # initialize the six linear transformations used in the neural network\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_mu = nn.Linear(z_dim, z_dim)\n",
    "        # modify the default initialization of lin_z_to_mu\n",
    "        # so that it's starts out as the identity function\n",
    "        self.lin_z_to_mu.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_mu.bias.data = torch.zeros(z_dim)\n",
    "        # initialize the three non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1):\n",
    "        \"\"\"\n",
    "        Given the latent `z_{t-1}` corresponding to the time step t-1\n",
    "        we return the mean and sigma vectors that parameterize the\n",
    "        (diagonal) gaussian distribution `p(z_t | z_{t-1})`\n",
    "        \"\"\"\n",
    "        # compute the gating function and one minus the gating function\n",
    "        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n",
    "        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\n",
    "        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\n",
    "        # compute the 'proposed mean'\n",
    "        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\n",
    "        # assemble the actual mean used to sample z_t, which mixes a linear transformation\n",
    "        # of z_{t-1} with the proposed mean modulated by the gating function\n",
    "        mu = one_minus_gate * self.lin_z_to_mu(z_t_1) + gate * proposed_mean\n",
    "        # compute the sigma used to sample z_t, using the proposed mean from above as input\n",
    "        # the softplus ensures that sigma is positive\n",
    "        sigma = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "        # return mu, sigma which can be fed into Normal\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "class Combiner(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes `q(z_t | z_{t-1}, x_{t:T})`, which is the basic building block\n",
    "    of the guide (i.e. the variational distribution). The dependence on `x_{t:T}` is\n",
    "    through the hidden state of the RNN (see the PyTorch module `rnn` below)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super(Combiner, self).__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.lin_hidden_to_mu = nn.Linear(rnn_dim, z_dim)\n",
    "        self.lin_hidden_to_sigma = nn.Linear(rnn_dim, z_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1, h_rnn):\n",
    "        \"\"\"\n",
    "        Given the latent z at at a particular time step t-1 as well as the hidden\n",
    "        state of the RNN `h(x_{t:T})` we return the mean and sigma vectors that\n",
    "        parameterize the (diagonal) gaussian distribution `q(z_t | z_{t-1}, x_{t:T})`\n",
    "        \"\"\"\n",
    "        # combine the rnn hidden state with a transformed version of z_t_1\n",
    "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
    "        # use the combined hidden state to compute the mean used to sample z_t\n",
    "        mu = self.lin_hidden_to_mu(h_combined)\n",
    "        # use the combined hidden state to compute the sigma used to sample z_t\n",
    "        sigma = self.softplus(self.lin_hidden_to_sigma(h_combined))\n",
    "        # return mu, sigma which can be fed into Normal\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "class DMM(nn.Module):\n",
    "    \"\"\"\n",
    "    This PyTorch Module encapsulates the model as well as the\n",
    "    variational distribution (the guide) for the Deep Markov Model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=88, z_dim=100, emission_dim=100,\n",
    "                 transition_dim=200, rnn_dim=600, rnn_dropout_rate=0.0,\n",
    "                 num_iafs=0, iaf_dim=50, use_cuda=False):\n",
    "        super(DMM, self).__init__()\n",
    "        # instantiate PyTorch modules used in the model and guide below\n",
    "        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n",
    "        self.trans = GatedTransition(z_dim, transition_dim)\n",
    "        self.combiner = Combiner(z_dim, rnn_dim)\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, nonlinearity='relu',\n",
    "                          batch_first=True, bidirectional=False, num_layers=1,\n",
    "                          dropout=rnn_dropout_rate)\n",
    "\n",
    "        # if we're using normalizing flows, instantiate those too\n",
    "        iafs = [InverseAutoregressiveFlow(z_dim, iaf_dim) for _ in range(num_iafs)]\n",
    "        self.iafs = nn.ModuleList(iafs)\n",
    "\n",
    "        # define a (trainable) parameters z_0 and z_q_0 that help define the probability\n",
    "        # distributions p(z_1) and q(z_1)\n",
    "        # (since for t = 1 there are no previous latents to condition on)\n",
    "        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        # define a (trainable) parameter for the initial hidden state of the rnn\n",
    "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        # if on gpu cuda-ize all PyTorch (sub)modules\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    # the model p(x_{1:T} | z_{1:T}) p(z_{1:T})\n",
    "    def model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        # this needs to happen in both the model and guide\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # set z_prev = z_0 to setup the recursive conditioning in p(z_t | z_{t-1})\n",
    "        z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n",
    "\n",
    "        # sample the latents z and observed x's one time step at a time\n",
    "        for t in range(1, T_max + 1):\n",
    "            # the next three lines of code sample z_t ~ p(z_t | z_{t-1})\n",
    "            # note that (both here and elsewhere) log_pdf_mask takes care of both\n",
    "            # (i)  KL annealing; and\n",
    "            # (ii) raggedness in the observed data (i.e. different sequences\n",
    "            #      in the mini-batch have different lengths)\n",
    "\n",
    "            # first compute the parameters of the diagonal gaussian distribution p(z_t | z_{t-1})\n",
    "            z_mu, z_sigma = self.trans(z_prev)\n",
    "            # then sample z_t according to dist.Normal(z_mu, z_sigma)\n",
    "            z_t = pyro.sample(\"z_%d\" % t,\n",
    "                              dist.normal,\n",
    "                              z_mu,\n",
    "                              z_sigma,\n",
    "                              log_pdf_mask=annealing_factor * mini_batch_mask[:, t - 1:t])\n",
    "\n",
    "            # compute the probabilities that parameterize the bernoulli likelihood\n",
    "            emission_probs_t = self.emitter(z_t)\n",
    "            # the next statement instructs pyro to observe x_t according to the\n",
    "            # bernoulli distribution p(x_t|z_t)\n",
    "            pyro.observe(\"obs_x_%d\" % t, dist.bernoulli, mini_batch[:, t - 1, :],\n",
    "                         emission_probs_t,\n",
    "                         log_pdf_mask=mini_batch_mask[:, t - 1:t])\n",
    "            # the latent sampled at this time step will be conditioned upon\n",
    "            # in the next time step so keep track of it\n",
    "            z_prev = z_t\n",
    "\n",
    "    # the guide q(z_{1:T} | x_{1:T}) (i.e. the variational distribution)\n",
    "    def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "        # to be in contiguous gpu memory\n",
    "        h_0_contig = self.h_0 if not self.use_cuda \\\n",
    "            else self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()\n",
    "        # push the observed x's through the rnn;\n",
    "        # rnn_output contains the hidden state at each time step\n",
    "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "        # reverse the time-ordering in the hidden state and un-pack it\n",
    "        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "        # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n",
    "        z_prev = self.z_q_0\n",
    "\n",
    "        # sample the latents z one time step at a time\n",
    "        for t in range(1, T_max + 1):\n",
    "            # the next two lines assemble the distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "            z_mu, z_sigma = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "            z_dist = dist.normal\n",
    "\n",
    "            # if we are using normalizing flows, we apply the sequence of transformations\n",
    "            # parameterized by self.iafs to the base distribution defined in the previous line\n",
    "            # to yield a transformed distribution that we use for q(z_t|...)\n",
    "            if self.iafs.__len__() > 0:\n",
    "                z_dist = TransformedDistribution(z_dist, self.iafs)\n",
    "            # sample z_t from the distribution z_dist\n",
    "            z_t = pyro.sample(\"z_%d\" % t,\n",
    "                              z_dist,\n",
    "                              z_mu,\n",
    "                              z_sigma,\n",
    "                              log_pdf_mask=annealing_factor * mini_batch_mask[:, t - 1:t])\n",
    "            # the latent sampled at this time step will be conditioned upon in the next time step\n",
    "            # so keep track of it\n",
    "            z_prev = z_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     parser = argparse.ArgumentParser(description=\"parse args\")\n",
    "#     parser.add_argument('-n', '--num-epochs', type=int, default=5000)\n",
    "#     parser.add_argument('-lr', '--learning-rate', type=float, default=0.0004)\n",
    "#     parser.add_argument('-b1', '--beta1', type=float, default=0.96)\n",
    "#     parser.add_argument('-b2', '--beta2', type=float, default=0.999)\n",
    "#     parser.add_argument('-cn', '--clip-norm', type=float, default=20.0)\n",
    "#     parser.add_argument('-lrd', '--lr-decay', type=float, default=0.99996)\n",
    "#     parser.add_argument('-wd', '--weight-decay', type=float, default=0.6)\n",
    "#     parser.add_argument('-mbs', '--mini-batch-size', type=int, default=20)\n",
    "#     parser.add_argument('-ae', '--annealing-epochs', type=int, default=1000)\n",
    "#     parser.add_argument('-maf', '--minimum-annealing-factor', type=float, default=0.1)\n",
    "#     parser.add_argument('-rdr', '--rnn-dropout-rate', type=float, default=0.1)\n",
    "#     parser.add_argument('-iafs', '--num-iafs', type=int, default=0)\n",
    "#     parser.add_argument('-id', '--iaf-dim', type=int, default=100)\n",
    "#     parser.add_argument('-cf', '--checkpoint-freq', type=int, default=0)\n",
    "#     parser.add_argument('-lopt', '--load-opt', type=str, default='')\n",
    "#     parser.add_argument('-lmod', '--load-model', type=str, default='')\n",
    "#     parser.add_argument('-sopt', '--save-opt', type=str, default='')\n",
    "#     parser.add_argument('-smod', '--save-model', type=str, default='')\n",
    "#     parser.add_argument('--cuda', action='store_true')\n",
    "#     parser.add_argument('-l', '--log', type=str, default='dmm.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from observations import jsb_chorales\n",
    "from os.path import join, exists\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "\n",
    "# this function processes the raw data; in particular it unsparsifies it\n",
    "def process_data(base_path, filename, T_max=160, min_note=21, note_range=88):\n",
    "    output = join(base_path, filename)\n",
    "    if exists(output):\n",
    "        return\n",
    "\n",
    "    print(\"processing raw polyphonic music data...\")\n",
    "    data = jsb_chorales(base_path)\n",
    "    processed_dataset = {}\n",
    "    for split, data_split in zip(['train', 'test', 'valid'], data):\n",
    "        processed_dataset[split] = {}\n",
    "        n_seqs = len(data_split)\n",
    "        processed_dataset[split]['sequence_lengths'] = np.zeros((n_seqs), dtype=np.int32)\n",
    "        processed_dataset[split]['sequences'] = np.zeros((n_seqs, T_max, note_range))\n",
    "        for seq in range(n_seqs):\n",
    "            seq_length = len(data_split[seq])\n",
    "            processed_dataset[split]['sequence_lengths'][seq] = seq_length\n",
    "            for t in range(seq_length):\n",
    "                note_slice = np.array(list(data_split[seq][t])) - min_note\n",
    "                slice_length = len(note_slice)\n",
    "                if slice_length > 0:\n",
    "                    processed_dataset[split]['sequences'][seq, t, note_slice] = np.ones((slice_length))\n",
    "    pickle.dump(processed_dataset, open(output, \"wb\"))\n",
    "    print(\"dumped processed data to %s\" % output)\n",
    "\n",
    "\n",
    "# this logic will be initiated upon import\n",
    "base_path = './data'\n",
    "process_data(base_path, \"jsb_processed.pkl\")\n",
    "\n",
    "\n",
    "# this function takes a numpy mini-batch and reverses each sequence\n",
    "# (w.r.t. the temporal axis, i.e. axis=1)\n",
    "def reverse_sequences_numpy(mini_batch, seq_lengths):\n",
    "    reversed_mini_batch = mini_batch.copy()\n",
    "    for b in range(mini_batch.shape[0]):\n",
    "        T = seq_lengths[b]\n",
    "        reversed_mini_batch[b, 0:T, :] = mini_batch[b, (T - 1)::-1, :]\n",
    "    return reversed_mini_batch\n",
    "\n",
    "\n",
    "# this function takes a torch mini-batch and reverses each sequence\n",
    "# (w.r.t. the temporal axis, i.e. axis=1)\n",
    "# in contrast to `reverse_sequences_numpy`, this function plays\n",
    "# nice with torch autograd\n",
    "def reverse_sequences_torch(mini_batch, seq_lengths):\n",
    "    reversed_mini_batch = mini_batch.new_zeros(mini_batch.size())\n",
    "    for b in range(mini_batch.size(0)):\n",
    "        T = seq_lengths[b]\n",
    "        time_slice = np.arange(T - 1, -1, -1)\n",
    "        time_slice = torch.cuda.LongTensor(time_slice) if 'cuda' in mini_batch.data.type() \\\n",
    "            else torch.LongTensor(time_slice)\n",
    "        reversed_sequence = torch.index_select(mini_batch[b, :, :], 0, time_slice)\n",
    "        reversed_mini_batch[b, 0:T, :] = reversed_sequence\n",
    "    return reversed_mini_batch\n",
    "\n",
    "\n",
    "# this function takes the hidden state as output by the PyTorch rnn and\n",
    "# unpacks it it; it also reverses each sequence temporally\n",
    "def pad_and_reverse(rnn_output, seq_lengths):\n",
    "    rnn_output, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
    "    reversed_output = reverse_sequences_torch(rnn_output, seq_lengths)\n",
    "    return reversed_output\n",
    "\n",
    "\n",
    "# this function returns a 0/1 mask that can be used to mask out a mini-batch\n",
    "# composed of sequences of length `seq_lengths`\n",
    "def get_mini_batch_mask(mini_batch, seq_lengths):\n",
    "    mask = np.zeros(mini_batch.shape[0:2])\n",
    "    for b in range(mini_batch.shape[0]):\n",
    "        mask[b, 0:seq_lengths[b]] = np.ones(seq_lengths[b])\n",
    "    return mask\n",
    "\n",
    "\n",
    "# this function prepares a mini-batch for training or evaluation.\n",
    "# it returns a mini-batch in forward temporal order (`mini_batch`) as\n",
    "# well as a mini-batch in reverse temporal order (`mini_batch_reversed`).\n",
    "# it also deals with the fact that packed sequences (which are what what we\n",
    "# feed to the PyTorch rnn) need to be sorted by sequence length.\n",
    "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):\n",
    "    # get the sequence lengths of the mini-batch\n",
    "    seq_lengths = seq_lengths[mini_batch_indices]\n",
    "    # sort the sequence lengths\n",
    "    sorted_seq_length_indices = np.argsort(seq_lengths)[::-1]\n",
    "    sorted_seq_lengths = seq_lengths[sorted_seq_length_indices]\n",
    "    sorted_mini_batch_indices = mini_batch_indices[sorted_seq_length_indices]\n",
    "\n",
    "    # compute the length of the longest sequence in the mini-batch\n",
    "    T_max = np.max(seq_lengths)\n",
    "    # this is the sorted mini-batch\n",
    "    mini_batch = sequences[sorted_mini_batch_indices, 0:T_max, :]\n",
    "    # this is the sorted mini-batch in reverse temporal order\n",
    "    mini_batch_reversed = reverse_sequences_numpy(mini_batch, sorted_seq_lengths)\n",
    "    # get mask for mini-batch\n",
    "    mini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)\n",
    "\n",
    "    # wrap in PyTorch Tensors, using default tensor type\n",
    "    mini_batch = torch.Tensor(mini_batch).type(torch.Tensor)\n",
    "    mini_batch_reversed = torch.Tensor(mini_batch_reversed).type(torch.Tensor)\n",
    "    mini_batch_mask = torch.Tensor(mini_batch_mask).type(torch.Tensor)\n",
    "\n",
    "    # cuda() here because need to cuda() before packing\n",
    "    if cuda:\n",
    "        mini_batch = mini_batch.cuda()\n",
    "        mini_batch_mask = mini_batch_mask.cuda()\n",
    "        mini_batch_reversed = mini_batch_reversed.cuda()\n",
    "\n",
    "    # do sequence packing\n",
    "    mini_batch_reversed = nn.utils.rnn.pack_padded_sequence(mini_batch_reversed,\n",
    "                                                            sorted_seq_lengths,\n",
    "                                                            batch_first=True)\n",
    "\n",
    "    return mini_batch, mini_batch_reversed, mini_batch_mask, sorted_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_file_loc = \"./data/jsb_processed.pkl\"\n",
    "# ingest training/validation/test data from disk\n",
    "data = pickle.load(open(jsb_file_loc, \"rb\"))\n",
    "training_seq_lengths = data['train']['sequence_lengths']\n",
    "training_data_sequences = data['train']['sequences']\n",
    "test_seq_lengths = data['test']['sequence_lengths']\n",
    "test_data_sequences = data['test']['sequences']\n",
    "val_seq_lengths = data['valid']['sequence_lengths']\n",
    "val_data_sequences = data['valid']['sequences']\n",
    "N_train_data = len(training_seq_lengths)\n",
    "N_train_time_slices = np.sum(training_seq_lengths)\n",
    "N_mini_batches = int(N_train_data / 20 +\n",
    "                     int(N_train_data % 20 > 0))\n",
    "\n",
    "\n",
    "# how often we do validation/test evaluation during training\n",
    "val_test_frequency = 50\n",
    "# the number of samples we use to do the evaluation\n",
    "n_eval_samples = 1\n",
    "\n",
    "# package repeated copies of val/test data for faster evaluation\n",
    "# (i.e. set us up for vectorization)\n",
    "def rep(x):\n",
    "    y = np.repeat(x, n_eval_samples, axis=0)\n",
    "    return y\n",
    "\n",
    "# get the validation/test data ready for the dmm: pack into sequences, etc.\n",
    "val_seq_lengths = rep(val_seq_lengths)\n",
    "test_seq_lengths = rep(test_seq_lengths)\n",
    "val_batch, val_batch_reversed, val_batch_mask, val_seq_lengths = get_mini_batch(np.arange(n_eval_samples * val_data_sequences.shape[0]), rep(val_data_sequences), val_seq_lengths)\n",
    "test_batch, test_batch_reversed, test_batch_mask, test_seq_lengths = get_mini_batch(\n",
    "    np.arange(n_eval_samples * test_data_sequences.shape[0]), rep(test_data_sequences),\n",
    "    test_seq_lengths)\n",
    "\n",
    "# instantiate the dmm\n",
    "dmm = DMM(rnn_dropout_rate=0.1, num_iafs=0,\n",
    "          iaf_dim=100)\n",
    "\n",
    "# setup optimizer\n",
    "adam_params = {\"lr\": 0.01, \"betas\": (0.96, 0.996),\n",
    "               \"clip_norm\": 10, \"lrd\": 0.99996,\n",
    "               \"weight_decay\": 0}\n",
    "adam = ClippedAdam(adam_params)\n",
    "\n",
    "# setup inference algorithm\n",
    "svi = SVI(dmm.model, dmm.guide, adam, \"ELBO\", trace_graph=False)\n",
    "\n",
    "# now we're going to define some functions we need to form the main training loop\n",
    "\n",
    "# saves the model and optimizer states to disk\n",
    "def save_checkpoint():\n",
    "    log(\"saving model to %s...\" % args.save_model)\n",
    "    torch.save(dmm.state_dict(), args.save_model)\n",
    "    log(\"saving optimizer states to %s...\" % args.save_opt)\n",
    "    adam.save(args.save_opt)\n",
    "    log(\"done saving model and optimizer checkpoints to disk.\")\n",
    "\n",
    "# loads the model and optimizer states from disk\n",
    "def load_checkpoint():\n",
    "    assert exists(args.load_opt) and exists(args.load_model), \\\n",
    "        \"--load-model and/or --load-opt misspecified\"\n",
    "    log(\"loading model from %s...\" % args.load_model)\n",
    "    dmm.load_state_dict(torch.load(args.load_model))\n",
    "    log(\"loading optimizer states from %s...\" % args.load_opt)\n",
    "    adam.load(args.load_opt)\n",
    "    log(\"done loading model and optimizer states.\")\n",
    "\n",
    "# prepare a mini-batch and take a gradient step to minimize -elbo\n",
    "def process_minibatch(epoch, which_mini_batch, shuffled_indices):\n",
    "    if 1000 > 0 and epoch < 1000:\n",
    "        # compute the KL annealing factor approriate for the current mini-batch in the current epoch\n",
    "        min_af = 0.1\n",
    "        annealing_factor = min_af + (1.0 - min_af) * \\\n",
    "            (float(which_mini_batch + epoch * N_mini_batches + 1) /\n",
    "             float(1000 * N_mini_batches))\n",
    "    else:\n",
    "        # by default the KL annealing factor is unity\n",
    "        annealing_factor = 1.0\n",
    "\n",
    "    # compute which sequences in the training set we should grab\n",
    "    mini_batch_start = (which_mini_batch * 20)\n",
    "    mini_batch_end = np.min([(which_mini_batch + 1) * 1000, N_train_data])\n",
    "    mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n",
    "    # grab a fully prepped mini-batch using the helper function in the data loader\n",
    "    mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n",
    "        = get_mini_batch(mini_batch_indices, training_data_sequences,\n",
    "                              training_seq_lengths)\n",
    "    \n",
    "    # do an actual gradient step\n",
    "    loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "                     mini_batch_seq_lengths, annealing_factor)\n",
    "#     loss = elbo.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "#                      mini_batch_seq_lengths, annealing_factor)\n",
    "    # keep track of the training loss\n",
    "    return loss\n",
    "\n",
    "# helper function for doing evaluation\n",
    "def do_evaluation():\n",
    "    # put the RNN into evaluation mode (i.e. turn off drop-out if applicable)\n",
    "    dmm.rnn.eval()\n",
    "\n",
    "    # compute the validation and test loss n_samples many times\n",
    "    val_nll = elbo.evaluate_loss(val_batch, val_batch_reversed, val_batch_mask,\n",
    "                                 val_seq_lengths) / np.sum(val_seq_lengths)\n",
    "    test_nll = elbo.evaluate_loss(test_batch, test_batch_reversed, test_batch_mask,\n",
    "                                  test_seq_lengths) / np.sum(test_seq_lengths)\n",
    "\n",
    "    # put the RNN back into training mode (i.e. turn on drop-out if applicable)\n",
    "    dmm.rnn.train()\n",
    "    return val_nll, test_nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e1a7b68af0b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# process each mini-batch; this is where we take gradient steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwhich_mini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_mini_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mepoch_nll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocess_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# report training diagnostics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-58388c641e4e>\u001b[0m in \u001b[0;36mprocess_minibatch\u001b[0;34m(epoch, which_mini_batch, shuffled_indices)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# do an actual gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n\u001b[0;32m---> 88\u001b[0;31m                      mini_batch_seq_lengths, annealing_factor)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;31m#     loss = elbo.step(mini_batch, mini_batch_reversed, mini_batch_mask,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m#                      mini_batch_seq_lengths, annealing_factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# get active params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhich_elbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_r\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0melbo_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0msurrogate_elbo_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mguide_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/poutine/trace_poutine.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/poutine/trace_poutine.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_INPUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                             args=args, kwargs=kwargs)\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTracePoutine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_RETURN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_RETURN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"return\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/poutine/poutine.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \"\"\"\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5aa400b34848>\u001b[0m in \u001b[0;36mguide\u001b[0;34m(self, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths, annealing_factor)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# push the observed x's through the rnn;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# rnn_output contains the hidden state at each time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_reversed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_0_contig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;31m# reverse the time-ordering in the hidden state and un-pack it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mrnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_and_reverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_seq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_acceptable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCudnnRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?"
     ]
    }
   ],
   "source": [
    "# if checkpoint files provided, load model and optimizer states from disk before we start training\n",
    "\n",
    "#################\n",
    "# TRAINING LOOP #\n",
    "#################\n",
    "times = [time.time()]\n",
    "for epoch in range(5000):\n",
    "    # accumulator for our estimate of the negative log likelihood (or rather -elbo) for this epoch\n",
    "    epoch_nll = 0.0\n",
    "    # prepare mini-batch subsampling indices for this epoch\n",
    "    shuffled_indices = np.arange(N_train_data)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    # process each mini-batch; this is where we take gradient steps\n",
    "    for which_mini_batch in range(N_mini_batches):\n",
    "        epoch_nll += process_minibatch(epoch, which_mini_batch, shuffled_indices)\n",
    "\n",
    "    # report training diagnostics\n",
    "    times.append(time.time())\n",
    "    epoch_time = times[-1] - times[-2]\n",
    "    log(\"[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)\" %\n",
    "        (epoch, epoch_nll / N_train_time_slices, epoch_time))\n",
    "\n",
    "    # do evaluation on test and validation data and report results\n",
    "    if val_test_frequency > 0 and epoch > 0 and epoch % val_test_frequency == 0:\n",
    "        val_nll, test_nll = do_evaluation()\n",
    "        log(\"[val/test epoch %04d]  %.4f  %.4f\" % (epoch, val_nll, test_nll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
